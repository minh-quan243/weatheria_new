import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from xgboost import XGBClassifier, plot_importance
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_score, recall_score, f1_score, roc_auc_score
)
from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize

# ========================
# ğŸ“¥ 1. Load dá»¯ liá»‡u
# ========================
df = pd.read_csv("D:/Pycharm/weather-new/data/Processed/Intensity/storm_intensity1_dataset.csv")

features = ['Rain', 'Temp', 'WindSpeed', 'Pressure', 'Humidity', 'CloudCover', 'WindDirection']
target = 'storm_category'

X = df[features]
y = df[target]

# ========================
# ğŸ”¤ 2. Encode nhÃ£n phÃ¢n loáº¡i
# ========================
le = LabelEncoder()
y_encoded = le.fit_transform(y)
class_labels = le.classes_
num_classes = len(class_labels)

# ========================
# âœ… 3. TÃ¡ch dá»¯ liá»‡u train/test
# ========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, stratify=y_encoded, test_size=0.2, random_state=42
)

# ========================
# ğŸ§¼ 4. Chuáº©n hÃ³a dá»¯ liá»‡u
# ========================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ========================
# ğŸ§  5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh XGBoost
# ========================
model = XGBClassifier(
    objective='multi:softmax',
    num_class=num_classes,
    eval_metric='mlogloss',
    max_depth=6,
    n_estimators=100,
    learning_rate=0.1,
    random_state=42
)
model.fit(X_train_scaled, y_train)

# ========================
# ğŸ“Š 6. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
# ========================
y_pred = model.predict(X_test_scaled)
y_test_true = le.inverse_transform(y_test)
y_pred_true = le.inverse_transform(y_pred)

print("ğŸ“ˆ Classification Report:\n", classification_report(y_test_true, y_pred_true))
print("âœ… Accuracy:", accuracy_score(y_test_true, y_pred_true))
print("ğŸ¯ Precision (macro):", precision_score(y_test_true, y_pred_true, average="macro"))
print("ğŸ” Recall (macro):", recall_score(y_test_true, y_pred_true, average="macro"))
print("ğŸ F1 Score (macro):", f1_score(y_test_true, y_pred_true, average="macro"))

# ğŸ¯ ROC-AUC vá»›i multi-class (One-vs-Rest)
y_test_bin = label_binarize(y_test, classes=range(num_classes))
y_proba = model.predict_proba(X_test_scaled)
auc = roc_auc_score(y_test_bin, y_proba, average="macro", multi_class="ovr")
print("ğŸ“‰ ROC-AUC (macro):", auc)

# ========================
# ğŸ”· 7. Ma tráº­n nháº§m láº«n
# ========================
plt.figure(figsize=(6, 4))
cm = confusion_matrix(y_test_true, y_pred_true, labels=class_labels)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Dá»± Ä‘oÃ¡n")
plt.ylabel("Thá»±c táº¿")
plt.tight_layout()
plt.show()

# ========================
# ğŸŒŸ 8. Feature Importance
# ========================
plt.figure(figsize=(8, 6))
plot_importance(model, importance_type='gain', show_values=False)
plt.title("Feature Importance (Gain) - XGBoost")
plt.tight_layout()
plt.show()

# ========================
# ğŸ’¾ 9. LÆ°u mÃ´ hÃ¬nh, encoder vÃ  scaler
# ========================
joblib.dump(model, "model_use/storm_intensity_xgboost.pkl")
joblib.dump(le, "model_use/label_encoder_intensity.pkl")
joblib.dump(scaler, "model_use/scaler_intensity.pkl")

print("âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh, label encoder vÃ  scaler vÃ o thÆ° má»¥c model_use/")
